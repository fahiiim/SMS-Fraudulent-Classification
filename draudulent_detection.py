# -*- coding: utf-8 -*-
"""Draudulent Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Wfi7MmHO4l4wB7E82BtaeeUrlJySOyu

# SMS Fraudulent Classification

# A. Load Dataset
"""

import pandas as pd

df = pd.read_csv("/content/sms_fraudulent.csv", encoding='latin-1')

df.head(100)

df.shape

"""# B. Preprocessing Dataset
* Clean the text data by removing unwanted characters, punctuation, and ensuring consistent casing.

* Text tokenization: Convert text into sequences of words or subwords.

* Label Encoding: Convert the labels (spam, ham) into numerical values (0 for ham, 1 for spam).
"""

# clean the text data by removing unwanted characters, punctions and ensuring consistent casing
import re
import string
from sklearn.preprocessing import LabelEncoder

#preprocessing function
def clean_text(text):
    text = text.lower()  # convert to lowercase
    text = re.sub(f"[{string.punctuation}]", " ", text)  # remove punctuation
    text = re.sub(r"\d+", " ", text)  # remove numbers
    text = text.strip()  # remove extra spaces
    return text

# clean the text column
df["text"] = df["text"].apply(clean_text)

# encode target labels, "ham = 0", "spam = 1"
le = LabelEncoder()
df["label"] = le.fit_transform(df["target"])

import random
# Get a random number of rows between 1 and 100
n_random_rows = random.randint(1, 100)
# Display a random sample of rows using the sample() method
display(df.sample(n=n_random_rows))

"""# tokenization and text-vectorization with transformers"""

!pip install tensorflow transformers

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

max_length = 64

def encode_text(text):
    tokens = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length= max_length,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        return_tensors="tf",
    )
encoded_text = df["text"].apply(lambda x : encode_text(x))
encoded_text[0]

"""# model definition and training"""

import tensorflow as tf
from transformers import TFBertForSequenceClassification

model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2, use_safetensors=False)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy("accuracy")],
    run_eagerly=True # Add this line
)

model.summary()

"""# prepare trainig and validation data"""

from sklearn.model_selection import train_test_split
import tensorflow as tf
from transformers import BertTokenizer

# Assuming the tokenizer is defined and max_length is set in a previous cell
# from transformers import BertTokenizer
# tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
# max_length = 64

def encode_text(text):
    tokens = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length= max_length,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        return_tensors="tf",
    )
    return tokens

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

# Tokenize the training and validation data
train_encodings = X_train.apply(lambda x: encode_text(x))
val_encodings = X_val.apply(lambda x: encode_text(x))

# Convert the Series of encoding dictionaries into dictionaries of arrays with string keys
def convert_to_dict_of_arrays(encodings):
    input_ids = tf.concat([x['input_ids'] for x in encodings], axis=0)
    attention_mask = tf.concat([x['attention_mask'] for x in encodings], axis=0)
    # Include token_type_ids if your model uses them and the tokenizer provides them
    # token_type_ids = tf.concat([x['token_type_ids'] for x in encodings], axis=0)
    return {'input_ids': input_ids, 'attention_mask': attention_mask}#, 'token_type_ids': token_type_ids}

train_input_dict = convert_to_dict_of_arrays(train_encodings)
val_input_dict = convert_to_dict_of_arrays(val_encodings)


# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((train_input_dict, y_train.values))
val_dataset = tf.data.Dataset.from_tensor_slices((val_input_dict, y_val.values))

# Batch the datasets
train_dataset = train_dataset.batch(16).shuffle(1000)
val_dataset = val_dataset.batch(16)

# train the model
history = model.fit(
    train_dataset,
    epochs=3,
    validation_data=val_dataset,
)

# Evaluate the model on the validation set
val_loss, val_accuracy = model.evaluate(val_dataset)
print(f'Validation Accuracy: {val_accuracy*100:.2f}%')

from sklearn.metrics import confusion_matrix, classification_report

# Predict on validation data
y_pred = model.predict(val_dataset)
y_pred_labels = tf.argmax(y_pred.logits, axis=1)

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_val, y_pred_labels)
print("Confusion Matrix:")
print(conf_matrix)

# Classification report
report = classification_report(y_val, y_pred_labels)
print(report)

# Save the trained model
model.save_pretrained("sms_spam_model")
tokenizer.save_pretrained("sms_spam_model")